import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import glob
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

# Function to load and combine multiple datasets based on time
def load_and_combine_data(file_paths, time_column):
    dfs = [pd.read_csv(file_path).rename(columns=lambda x: x.strip()) for file_path in file_paths]  # Clean column names
    combined_df = dfs[0]
    for df in dfs[1:]:
        combined_df = pd.merge(combined_df, df, on=time_column, how='inner')
    return combined_df

# Function to preprocess the data (scaling and creating sliding windows)
def preprocess_data(df, input_columns, output_columns, window_size):
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
    
    X = []
    y = []
    for i in range(len(df) - window_size):
        X.append(df_scaled[input_columns].iloc[i:i + window_size].values)
        y.append(df_scaled[output_columns].iloc[i + window_size].values)
    
    X = np.array(X)
    y = np.array(y)
    
    return X, y, scaler

# Function to split data into training, validation, and test sets
def split_data(X, y, train_size, val_size, test_size):
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_size), random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_size/(val_size+test_size)), random_state=42)
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# Define the neural network model with more layers
class FullyConnectedNN(nn.Module):
    def __init__(self, input_size):
        super(FullyConnectedNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 2)  # Predicting Cd and Cl
        
    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        x = self.fc5(x)
        return x

# Function to train the model with early stopping and batch processing
def train_model(model, train_loader, val_loader, epochs, lr, patience):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    train_losses = []
    val_losses = []
    best_val_loss = np.inf
    patience_counter = 0
    
    for epoch in range(epochs):
        # Training
        model.train()
        running_train_loss = 0.0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            y_pred = model(X_batch)
            train_loss = criterion(y_pred, y_batch)
            train_loss.backward()
            optimizer.step()
            running_train_loss += train_loss.item()
        
        train_losses.append(running_train_loss / len(train_loader))
        
        # Validation
        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                y_val_pred = model(X_val_batch)
                val_loss = criterion(y_val_pred, y_val_batch)
                running_val_loss += val_loss.item()
        
        val_loss_avg = running_val_loss / len(val_loader)
        val_losses.append(val_loss_avg)
        
        # Early stopping check
        if val_loss_avg < best_val_loss:
            best_val_loss = val_loss_avg
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch+1}')
            break
        
        if (epoch+1) % 10 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')
    
    return train_losses, val_losses

# Function to plot loss and predictions
def plot_results(train_losses, val_losses, y_test, y_test_pred):
    # Plot loss over epochs
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.legend()
    plt.show()

    # Scatter plot of predicted vs actual Cd and Cl
    plt.figure(figsize=(10, 5))
    plt.scatter(y_test[:, 0], y_test_pred[:, 0], label='Cd', color='blue', alpha=0.6)
    plt.scatter(y_test[:, 1], y_test_pred[:, 1], label='Cl', color='red', alpha=0.6)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Predicted vs Actual Cd and Cl (Scatter)')
    plt.legend()
    plt.show()

    # Plot of predictions vs actual values for Cd and Cl
    plt.figure(figsize=(10, 5))
    plt.plot(y_test[:, 0], label='Actual Cd', color='blue')
    plt.plot(y_test_pred[:, 0], label='Predicted Cd', color='cyan')
    plt.xlabel('Test Sample Index')
    plt.ylabel('Cd Value')
    plt.title('Predicted vs Actual Cd')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(y_test[:, 1], label='Actual Cl', color='red')
    plt.plot(y_test_pred[:, 1], label='Predicted Cl', color='orange')
    plt.xlabel('Test Sample Index')
    plt.ylabel('Cl Value')
    plt.title('Predicted vs Actual Cl')
    plt.legend()
    plt.show()

# Function to print predicted values
def print_predictions(y_test, y_test_pred):
    print("\nPredicted vs Actual Values:")
    for i in range(len(y_test)):
        print(f"Test {i+1}: Actual Cd: {y_test[i, 0]:.4f}, Predicted Cd: {y_test_pred[i, 0]:.4f}, "
              f"Actual Cl: {y_test[i, 1]:.4f}, Predicted Cl: {y_test_pred[i, 1]:.4f}")

# Execution starts here
file_paths = ['D:\Research Project\Analysis-of-neural-network-architectures-for-predicting-time-series-data-from-simulations/rotational_speed.csv', 'D:\Research Project\Analysis-of-neural-network-architectures-for-predicting-time-series-data-from-simulations/test_data.csv']  # Add your file paths
time_column = 'Time'  # Assuming time is the column to merge on
input_columns = ['Rotational Speed (units)_file1', 'Cd_file1', 'Cl_file1', 
                 'Rotational Speed (units)_file2', 'Cd_file2', 'Cl_file2']  # Input features
output_columns = ['Cd_file1', 'Cl_file1']  # Output features
window_size = 5
train_size = 0.7
val_size = 0.15
test_size = 0.15
epochs = 80
lr = 0.0001
batch_size = 32
patience = 10

# Load and combine data
df = load_and_combine_data(file_paths, time_column)

# Preprocess data
X, y, scaler = preprocess_data(df, input_columns, output_columns, window_size)

# Split data into train, validation, and test sets
X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, train_size, val_size, test_size)

# Convert data to torch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Create DataLoader for batch processing
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Initialize model
model = FullyConnectedNN(X_train.shape[1] * X_train.shape[2])

# Train the model
train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr, patience=patience)

# Evaluate the model on test data
model.eval()
with torch.no_grad():
    y_test_pred = model(X_test).detach().numpy()

y_test = y_test.detach().numpy()

# Plot results
plot_results(train_losses, val_losses, y_test, y_test_pred)

# Print
print_predictions(y_test, y_test_pred)








